{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conditional Generative Adversarial Network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMs5cuZ1L3KEuQ9pdv1Pi1u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiransiriwardhana/FYP_Conditional_GAN/blob/main/Conditional_Generative_Adversarial_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hRn5BoL9bep"
      },
      "source": [
        "import os\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "else:\n",
        "  TF_MASTER=''\n",
        "\n",
        "tpu_address = TF_MASTER"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asjjOXR99jRF",
        "outputId": "2cf74923-ccc7-4cb2-d1d7-cd1dd827fe09"
      },
      "source": [
        "import tensorflow as tf\n",
        "# %tensorflow_version 2.2.0\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_address)\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "print(\"Number of devices: \", len(tf.config.list_logical_devices('TPU')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.89.54.66:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.89.54.66:8470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KovoyOLy9llw"
      },
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq8dXGI49tAT"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LAZX7NI9wCc"
      },
      "source": [
        "import os\n",
        "\n",
        "len(os.listdir('/content/drive/MyDrive/binarized_images/'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVvVIzSo95jC"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def add_gaussian_noise(img):\n",
        "\n",
        "    # data_ = normalise(data, (-1, 1), data_range)\n",
        "    # noisy = data_ + np.random.normal(mean, stdev, data.shape)\n",
        "    # noisy = np.clip(noisy, -1, 1) if clip else noisy\n",
        "    # return normalise(noisy, data_range, (-1, 1))\n",
        "\n",
        "    VARIABILITY = 50\n",
        "    deviation  = VARIABILITY * random.random()\n",
        "    noise = np.random.normal(0, deviation, img.shape)\n",
        "    img += noise\n",
        "    np.clip(img, 0., 255.)\n",
        "    return img\n",
        "\n",
        "    # return noisy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2DT8QvZ9_1F"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "\n",
        "def normalize(data, new_range=(-1, 1), current_range=None, axis=None):\n",
        "  \n",
        "    s = new_range[1] - new_range[0]\n",
        "    if current_range is not None:\n",
        "        mins = current_range[0]\n",
        "        maxs = current_range[1]\n",
        "    elif axis is not None:\n",
        "        mins = np.nanmin(data, axis=axis, keepdims=True)\n",
        "        maxs = np.nanmax(data, axis=axis, keepdims=True)   \n",
        "    else:\n",
        "        mins = data.min()\n",
        "        maxs = data.max() \n",
        "    return s * (data - mins) / (maxs - mins) + new_range[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIqnLCmb-Ejd"
      },
      "source": [
        "import cv2\n",
        "import random\n",
        "\n",
        "folder_name = os.listdir('/content/drive/MyDrive/binarized_images/')\n",
        "X = list()\n",
        "y = list()\n",
        "for i in range(len(folder_name)):\n",
        "  img = cv2.imread('/content/drive/MyDrive/binarized_images/'+str(i)+\".jpg\")\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  img = cv2.resize(img, dsize=(150, 150), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "  img_ = img.astype('float32')\n",
        "  y_img = normalize(img_)\n",
        "  y.append(y_img)\n",
        "  noisy = normalize(add_gaussian_noise(img_))\n",
        "  X.append(noisy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVJgmARQ-IkG"
      },
      "source": [
        "test_X = list()\n",
        "for i in range(len(X)):\n",
        "  if (i == 431):\n",
        "    break\n",
        "  else:\n",
        "    test_X.append(X[i])\n",
        "    X.pop(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zQNuKLl-PR1"
      },
      "source": [
        "test_y = list()\n",
        "for i in range(len(y)):\n",
        "  if (i == 431):\n",
        "    break\n",
        "  else:\n",
        "    test_y.append(y[i])\n",
        "    y.pop(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSI9BqeD-TNT"
      },
      "source": [
        "train_X = X \n",
        "train_y = y\n",
        "\n",
        "train_X = np.array(train_X)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjtln1cX-Vqa"
      },
      "source": [
        "train_y = np.array(train_y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-flYz0l-YKp"
      },
      "source": [
        "test_X = np.array(test_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OklvXwKZ-a6w"
      },
      "source": [
        "\n",
        "test_y = np.array(test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0Wp8nD9-f8S"
      },
      "source": [
        "print(\"train_X shape: \", train_X.shape)\n",
        "print(\"train_y shape: \", train_y.shape)\n",
        "print(\"test_X shape: \", test_X.shape)\n",
        "print(\"test_y shape\", test_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blbzkGU1-jlJ"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "img_f = train_X[6]\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(cv2.cvtColor(img_f, cv2.COLOR_BGR2RGB), interpolation='nearest')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXodbXJl-m-O"
      },
      "source": [
        "img_f = train_y[6]\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(cv2.cvtColor(img_f, cv2.COLOR_BGR2RGB), interpolation='nearest')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbAwf3h5-rH9"
      },
      "source": [
        "train_X = np.expand_dims(train_X, axis=3)\n",
        "train_y = np.expand_dims(train_y, axis=3)\n",
        "test_X = np.expand_dims(test_X, axis=3)\n",
        "test_y = np.expand_dims(test_y, axis=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R0khggs-uBj"
      },
      "source": [
        "print(\"train_X shape: \", train_X.shape)\n",
        "print(\"train_y shape: \", train_y.shape)\n",
        "print(\"test_X shape: \", test_X.shape)\n",
        "print(\"test_y shape\", test_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ2qzEV2-uQX"
      },
      "source": [
        "\n",
        "\n",
        "from os.path import join\n",
        "\n",
        "class ConfigCGAN:\n",
        "    \"\"\"\n",
        "    Configuration parameters for the Conditional GAN\n",
        "    \"\"\"\n",
        "    # Dimensions\n",
        "    raw_size = 150\n",
        "    adjust_size = 150\n",
        "    train_size = 150\n",
        "    channels = 1\n",
        "    base_number_of_filters = 64\n",
        "    kernel_size = (3, 3)\n",
        "    strides = (2, 2)\n",
        "\n",
        "    # Fixed model parameters\n",
        "    leak = 0.2\n",
        "    dropout_rate = 0.5\n",
        "\n",
        "    # Hyperparameters\n",
        "    learning_rate = 2e-4\n",
        "    beta1 = 0.5\n",
        "    max_epoch = 300\n",
        "    L1_lambda = 100\n",
        "\n",
        "    # Data\n",
        "    buffer_size = 1500\n",
        "    batch_size = 50\n",
        "\n",
        "    # Data storage\n",
        "    save_per_epoch = 5\n",
        "    exp_name = 'noise_gan'\n",
        "    data_path = join('/content/drive/MyDrive/out', exp_name, 'data')\n",
        "    model_path = join('/content/drive/MyDrive/out', exp_name, 'model')\n",
        "    results_path = join('/content/drive/MyDrive/out', exp_name, 'results')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HrYND65-xPc"
      },
      "source": [
        "config = ConfigCGAN()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxf97O_H-5QE"
      },
      "source": [
        "def next_power_2(n):\n",
        "    \"\"\"\n",
        "    Compute the nearest power of 2 greater than n.\n",
        "    Arguments:\n",
        "        n: integer.\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    # If it is a non-zero power of 2, return it\n",
        "    if n and not (n & (n - 1)): \n",
        "        return n \n",
        "    # Keep dividing n by 2 until it is 0\n",
        "    while n != 0:  \n",
        "        n >>= 1\n",
        "        count += 1\n",
        "    # Result is 2 to the power of divisions taken\n",
        "    return 1 << count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B2Ge-xX--Al"
      },
      "source": [
        "def padding_power_2(shape):\n",
        "    \"\"\"\n",
        "    Get the padding required to change the given shape to a square power \n",
        "    of 2 in each dimension.\n",
        "    Arguments:\n",
        "        shape: tuple of 2 ints. The original shape.\n",
        "    \"\"\"\n",
        "    padded_size = next_power_2(max(shape))\n",
        "    return ((padded_size - shape[0])//2, (padded_size - shape[1])//2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzZOhiXd_Aoy"
      },
      "source": [
        "def mse(x1, x2, norm=2):\n",
        "    return tf.reduce_mean(tf.square((x1 - x2) / norm))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXqXQOGd_EB2"
      },
      "source": [
        "def rmse(x1, x2, norm=2):\n",
        "    return tf.sqrt(mse(x1, x2, norm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLMjsGoj_HuM"
      },
      "source": [
        "def psnr(x1, x2, max_diff=1):\n",
        "    return 20. * tf.math.log(max_diff / rmse(x1, x2)) / tf.math.log(10.)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q22ZyjR_Mnb"
      },
      "source": [
        "def make_generator_model():\n",
        "    f = config.base_number_of_filters\n",
        "    k = config.kernel_size\n",
        "    s = config.strides\n",
        "    sz = config.train_size\n",
        "    c = config.channels\n",
        "    pad = padding_power_2((sz, sz))\n",
        "\n",
        "    if sz <= 128:\n",
        "        raise RuntimeError(\"Input size must be larger than 128 for this U-Net model\")\n",
        "\n",
        "    inputs = tf.keras.layers.Input((sz, sz, c), name=\"ginput\")\n",
        "    # scaled_input = tf.keras.layers.Rescaling(scale=1./127.5, offset=-1)(inputs)\n",
        "    inputs_pad = tf.keras.layers.ZeroPadding2D(pad, name=\"gpad\")(inputs)\n",
        "\n",
        "    # Encoder layers\n",
        "    # Input is sz x sz x c\n",
        "    ge1 = tf.keras.layers.Conv2D(f, k, s, padding=\"same\", name=\"geconv1\")(inputs_pad)\n",
        "    # Input is sz2 x sz2 x f\n",
        "    ge2 = tf.keras.layers.LeakyReLU(config.leak, name=\"geact1\")(ge1)\n",
        "    ge2 = tf.keras.layers.Conv2D(2*f, k, s, padding=\"same\", name=\"geconv2\")(ge2)\n",
        "    ge2 = tf.keras.layers.BatchNormalization(name=\"gebn2\")(ge2)\n",
        "    # Input is sz4 x sz4 x 2f\n",
        "    ge3 = tf.keras.layers.LeakyReLU(config.leak, name=\"geact2\")(ge2)\n",
        "    ge3 = tf.keras.layers.Conv2D(4*f, k, s, padding=\"same\", name=\"geconv3\")(ge3)\n",
        "    ge3 = tf.keras.layers.BatchNormalization(name=\"gebn3\")(ge3)\n",
        "    # Input is sz8 x sz8 x 4f\n",
        "    ge4 = tf.keras.layers.LeakyReLU(config.leak, name=\"geact3\")(ge3)\n",
        "    ge4 = tf.keras.layers.Conv2D(8*f, k, s, padding=\"same\", name=\"geconv4\")(ge4)\n",
        "    ge4 = tf.keras.layers.BatchNormalization(name=\"gebn4\")(ge4)\n",
        "    # Input is sz16 x sz16 x 8f\n",
        "    ge5 = tf.keras.layers.LeakyReLU(config.leak, name=\"geact4\")(ge4)\n",
        "    ge5 = tf.keras.layers.Conv2D(8*f, k, s, padding=\"same\", name=\"geconv5\")(ge5)\n",
        "    ge5 = tf.keras.layers.BatchNormalization(name=\"gebn5\")(ge5)\n",
        "    # Input is sz32 x sz32 x 8f\n",
        "    ge6 = tf.keras.layers.LeakyReLU(config.leak, name=\"geact5\")(ge5)\n",
        "    ge6 = tf.keras.layers.Conv2D(8*f, k, s, padding=\"same\", name=\"geconv6\")(ge6)\n",
        "    ge6 = tf.keras.layers.BatchNormalization(name=\"gebn6\")(ge6)\n",
        "    # Input is sz64 x sz64 x 8f\n",
        "    ge7 = tf.keras.layers.LeakyReLU(config.leak, name=\"geact6\")(ge6)\n",
        "    ge7 = tf.keras.layers.Conv2D(8*f, k, s, padding=\"same\", name=\"geconv7\")(ge7)\n",
        "    ge7 = tf.keras.layers.BatchNormalization(name=\"gebn7\")(ge7)\n",
        "    # Input is sz128 x sz128 x 8f\n",
        "    ge8 = tf.keras.layers.LeakyReLU(config.leak, name=\"geact7\")(ge7)\n",
        "    ge8 = tf.keras.layers.Conv2D(8*f, k, s, padding=\"same\", name=\"geconv8\")(ge8)\n",
        "    ge8 = tf.keras.layers.BatchNormalization(name=\"gebn8\")(ge8)\n",
        "    # Input is sz256 x sz256 x 8f\n",
        "\n",
        "    # Decoder layers with skip connections\n",
        "    gd1 = tf.keras.layers.LeakyReLU(0.0, name=\"geact8\")(ge8)\n",
        "    gd1 = tf.keras.layers.Conv2DTranspose(8*f, k, s, padding=\"same\", name=\"gdconv1\")(gd1)\n",
        "    gd1 = tf.keras.layers.BatchNormalization(name=\"gdbn1\")(gd1)\n",
        "    gd1 = tf.keras.layers.Dropout(config.dropout_rate, name=\"gddrop1\")(gd1)\n",
        "    # Input is sz128 x sz128 x 8f\n",
        "    gd1 = tf.keras.layers.concatenate([gd1, ge7], axis=3, name=\"gdcat1\")\n",
        "    gd2 = tf.keras.layers.LeakyReLU(0.0, name=\"gdact1\")(gd1)\n",
        "    gd2 = tf.keras.layers.Conv2DTranspose(8*f, k, s, padding=\"same\", name=\"gdconv2\")(gd2)\n",
        "    gd2 = tf.keras.layers.BatchNormalization(name=\"gdbn2\")(gd2)\n",
        "    gd2 = tf.keras.layers.Dropout(config.dropout_rate, name=\"gddrop2\")(gd2)\n",
        "    # Input is sz64 x sz64 x 8f\n",
        "    gd2 = tf.keras.layers.concatenate([gd2, ge6], axis=3, name=\"gdcat2\")\n",
        "    gd3 = tf.keras.layers.LeakyReLU(0.0, name=\"gdact2\")(gd2)\n",
        "    gd3 = tf.keras.layers.Conv2DTranspose(8*f, k, s, padding=\"same\", name=\"gdconv3\")(gd3)\n",
        "    gd3 = tf.keras.layers.BatchNormalization(name=\"gdbn3\")(gd3)\n",
        "    gd3 = tf.keras.layers.Dropout(config.dropout_rate, name=\"gddrop3\")(gd3)\n",
        "    # Input is sz32 x sz32 x 8f\n",
        "    gd3 = tf.keras.layers.concatenate([gd3, ge5], axis=3, name=\"gdcat3\")\n",
        "    gd4 = tf.keras.layers.LeakyReLU(0.0, name=\"gdact3\")(gd3)\n",
        "    gd4 = tf.keras.layers.Conv2DTranspose(8*f, k, s, padding=\"same\", name=\"gdconv4\")(gd4)\n",
        "    gd4 = tf.keras.layers.BatchNormalization(name=\"gdbn4\")(gd4)\n",
        "    # Input is sz16 x sz16 x 8f\n",
        "    gd4 = tf.keras.layers.concatenate([gd4, ge4], axis=3, name=\"gdcat4\")\n",
        "    gd5 = tf.keras.layers.LeakyReLU(0.0, name=\"gdact4\")(gd4)\n",
        "    gd5 = tf.keras.layers.Conv2DTranspose(4*f, k, s, padding=\"same\", name=\"gdconv5\")(gd5)\n",
        "    gd5 = tf.keras.layers.BatchNormalization(name=\"gdbn5\")(gd5)\n",
        "    gd5 = tf.keras.layers.concatenate([gd5, ge3], axis=3, name=\"gdcat5\")\n",
        "    # Input is sz8 x sz8 x 4f\n",
        "    gd6 = tf.keras.layers.LeakyReLU(0.0, name=\"gdact5\")(gd5)\n",
        "    gd6 = tf.keras.layers.Conv2DTranspose(2*f, k, s, padding=\"same\", name=\"gdconv6\")(gd6)\n",
        "    gd6 = tf.keras.layers.BatchNormalization(name=\"gdbn6\")(gd6)\n",
        "    # Input is sz4 x sz4 x 2f\n",
        "    gd6 = tf.keras.layers.concatenate([gd6, ge2], axis=3, name=\"gdcat6\")\n",
        "    gd7 = tf.keras.layers.LeakyReLU(0.0, name=\"gdact6\")(gd6)\n",
        "    gd7 = tf.keras.layers.Conv2DTranspose(f, k, s, padding=\"same\", name=\"gdconv7\")(gd7)\n",
        "    gd7 = tf.keras.layers.BatchNormalization(name=\"gdbn7\")(gd7)\n",
        "    # Input is sz2 x sz2 x f\n",
        "    gd7 = tf.keras.layers.concatenate([gd7, ge1], axis=3, name=\"gdcat7\")\n",
        "    gd8 = tf.keras.layers.LeakyReLU(0.0)(gd7)\n",
        "    gd8 = tf.keras.layers.Conv2DTranspose(c, k, s, padding=\"same\", activation=\"tanh\", \n",
        "                          name=\"gdconvout\")(gd8)\n",
        "    # Input is sz x sz x nc\n",
        "    outputs = tf.keras.layers.Cropping2D(pad, name=\"gcrop\")(gd8)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"cond_gen\")\n",
        "  \n",
        "    return model\n",
        "\n",
        "\n",
        "def make_generator_model_small():\n",
        "    f = config.base_number_of_filters\n",
        "    k = config.kernel_size\n",
        "    s = config.strides\n",
        "    sz = config.train_size\n",
        "    c = config.channels\n",
        "    pad = padding_power_2((sz, sz))\n",
        "\n",
        "    inputs = tf.keras.layers.Input((sz, sz, c), name=\"ginput\")\n",
        "    # scaled_input = tf.keras.layers.Rescaling(scale=1./127.5, offset=-1)(inputs)\n",
        "    inputs_pad = tf.keras.layers.ZeroPadding2D(pad, name=\"gpad\")(inputs)\n",
        "\n",
        "    # Encoder layers\n",
        "    ge1 = tf.keras.layers.Conv2D(f, k, s, padding=\"same\", name=\"geconv1\")(inputs_pad)\n",
        "    \n",
        "    ge2 = tf.keras.layers.LeakyReLU(config.leak, name=\"geact1\")(ge1)\n",
        "    ge2 = tf.keras.layers.Conv2D(2*f, k, s, padding=\"same\", name=\"geconv2\")(ge2)\n",
        "    ge2 = tf.keras.layers.BatchNormalization(name=\"gebn2\")(ge2)\n",
        "    \n",
        "    ge3 = tf.keras.layers.LeakyReLU(config.leak, name=\"geact2\")(ge2)\n",
        "    ge3 = tf.keras.layers.Conv2D(4*f, k, s, padding=\"same\", name=\"geconv3\")(ge3)\n",
        "    ge3 = tf.keras.layers.BatchNormalization(name=\"gebn3\")(ge3)\n",
        "\n",
        "    ge4 = tf.keras.layers.LeakyReLU(config.leak, name=\"geact3\")(ge3)\n",
        "    ge4 = tf.keras.layers.Conv2D(8*f, k, s, padding=\"same\", name=\"geconv4\")(ge4)\n",
        "    ge4 = tf.keras.layers.BatchNormalization(name=\"gebn4\")(ge4)\n",
        "\n",
        "    # Decoder layers with skip connections\n",
        "    gd1 = tf.keras.layers.LeakyReLU(0.0, name=\"geact4\")(ge4)\n",
        "    # TODO not sure if dimensions need specifying\n",
        "    gd1 = tf.keras.layers.Conv2DTranspose(4*f, k, s, padding=\"same\", name=\"gdconv1\")(gd1)\n",
        "    gd1 = tf.keras.layers.BatchNormalization(name=\"gdbn1\")(gd1)\n",
        "    gd1 = tf.keras.layers.Dropout(config.dropout_rate, name=\"gddrop1\")(gd1)\n",
        "    gd1 = tf.keras.layers.concatenate([gd1, ge3], axis=3, name=\"gdcat1\")\n",
        "\n",
        "    gd2 = tf.keras.layers.LeakyReLU(0.0, name=\"gdact1\")(gd1)\n",
        "    gd2 = tf.keras.layers.Conv2DTranspose(2*f, k, s, padding=\"same\", name=\"gdconv2\")(gd2)\n",
        "    gd2 = tf.keras.layers.BatchNormalization(name=\"gdbn2\")(gd2)\n",
        "    gd2 = tf.keras.layers.Dropout(config.dropout_rate, name=\"gddrop2\")(gd2)\n",
        "    gd2 = tf.keras.layers.concatenate([gd2, ge2], axis=3, name=\"gdcat2\")\n",
        "\n",
        "    gd3 = tf.keras.layers.LeakyReLU(0.0, name=\"gdact2\")(gd2)\n",
        "    gd3 = tf.keras.layers.Conv2DTranspose(f, k, s, padding=\"same\", name=\"gdconv3\")(gd3)\n",
        "    gd3 = tf.keras.layers.BatchNormalization(name=\"gdbn3\")(gd3)\n",
        "    gd3 = tf.keras.layers.Dropout(config.dropout_rate, name=\"gddrop3\")(gd3)\n",
        "    gd3 = tf.keras.layers.concatenate([gd3, ge1], axis=3, name=\"gdcat3\")\n",
        "\n",
        "    gd4 = tf.keras.layers.LeakyReLU(0.0)(gd3)\n",
        "    gd4 = tf.keras.layers.Conv2DTranspose(c, k, s, padding=\"same\", activation=\"tanh\", \n",
        "                          name=\"gdconvout\")(gd4)\n",
        "    \n",
        "    outputs = tf.keras.layers.Cropping2D(pad, name=\"gcrop\")(gd4)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"cond_gen\")\n",
        "  \n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doAotDTp_Qgx"
      },
      "source": [
        "generator = make_generator_model_small()\n",
        "tf.keras.utils.plot_model(generator, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SglMIyAA_VKC"
      },
      "source": [
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htztkmnV_YSN"
      },
      "source": [
        "generator_large = make_generator_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8shIz_I_ap1"
      },
      "source": [
        "tf.keras.utils.plot_model(generator_large, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04y_SIPy_d_v"
      },
      "source": [
        "generator_large.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR3Y0jtK_h_Q"
      },
      "source": [
        "def make_discriminator_model():\n",
        "    f = config.base_number_of_filters\n",
        "    k = config.kernel_size\n",
        "    s = config.strides\n",
        "    sz = config.train_size\n",
        "    c = config.channels\n",
        "\n",
        "    inputs = tf.keras.layers.Input((sz, sz, c), name=\"dinput\")\n",
        "    # scaled_input = tf.keras.layers.Rescaling(scale=1./127.5, offset=-1)(inputs)\n",
        "    d0 = tf.keras.layers.Conv2D(f, k, s, padding=\"same\", name=\"dconv0\")(inputs)\n",
        "    d0 = tf.keras.layers.LeakyReLU(config.leak, name=\"dact0\")(d0)\n",
        "    \n",
        "    d1 = tf.keras.layers.Conv2D(2*f, k, s, padding=\"same\", name=\"dconv1\")(d0)\n",
        "    d1 = tf.keras.layers.BatchNormalization(name=\"dbn1\")(d1)\n",
        "    d1 = tf.keras.layers.LeakyReLU(config.leak, name=\"dact1\")(d1)\n",
        "\n",
        "    d2 = tf.keras.layers.Conv2D(4*f, k, s, padding=\"same\", name=\"dconv2\")(d1)\n",
        "    d2 = tf.keras.layers.BatchNormalization(name=\"dbn2\")(d2)\n",
        "    d2 = tf.keras.layers.LeakyReLU(config.leak, name=\"dact2\")(d2)\n",
        "\n",
        "    d3 = tf.keras.layers.Conv2D(8*f, k, s, padding=\"same\", name=\"dconv3\")(d2)\n",
        "    d3 = tf.keras.layers.BatchNormalization(name=\"dbn3\")(d3)\n",
        "    d3 = tf.keras.layers.LeakyReLU(config.leak, name=\"dact3\")(d3)\n",
        "\n",
        "    d4 = tf.keras.layers.Flatten(name=\"dflatout\")(d3)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(1, name=\"ddenseout\")(d4)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"cond_dsc\")\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7y_RTIz_lCb"
      },
      "source": [
        "discriminator = make_discriminator_model()\n",
        "tf.keras.utils.plot_model(discriminator, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qyqIcDx_pSo"
      },
      "source": [
        "discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeLA6-bX_sd7"
      },
      "source": [
        "rmse_list = list()\n",
        "psnr_list = list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IhRhrOL_utr"
      },
      "source": [
        "import random\n",
        "def generate_and_save_images(model, epoch, test_inputs, test_labels):\n",
        "    if model is None:\n",
        "        predictions = test_inputs\n",
        "    else:\n",
        "        # Make sure the training parameter is set to False because we\n",
        "        # don't want to train the batchnorm layer when doing inference.\n",
        "        predictions = model(test_inputs, training=False)\n",
        "\n",
        "    types = [predictions, test_labels]  # Image types (alternated in rows)\n",
        "    ntype = len(types)\n",
        "    nrows = 4\n",
        "    ncols = 8\n",
        "    fig = plt.figure(figsize=(8, 5))\n",
        "    \n",
        "    for i in range(ntype * predictions.shape[0]):\n",
        "        plt.subplot(nrows, ncols, i+1)\n",
        "        # Get relative index\n",
        "        row = int(i / ncols)\n",
        "        row_rel = row % ntype\n",
        "        group = int(row / ntype)\n",
        "        shift = ncols * (group * (ntype - 1) + row_rel)\n",
        "        idx = i - shift\n",
        "        # Plot\n",
        "        for t in range(ntype):\n",
        "            if row_rel == 0:\n",
        "                j = int(i / ntype)\n",
        "                rmse_ = rmse(test_labels[j], predictions[j], norm=2)\n",
        "                psnr_ = psnr(test_labels[j], predictions[j], max_diff=1)\n",
        "\n",
        "                rmse_list.append(rmse_)\n",
        "                psnr_list.append(psnr_)\n",
        "                \n",
        "                plt.xlabel('RMSE={:.3f}\\nPSNR={:.2f}'.format(rmse_, psnr_), fontsize=8)\n",
        "            if row_rel == t:\n",
        "                plt.imshow(types[row_rel][idx, :, :, 0], vmin=-1, vmax=1, cmap='gray')\n",
        "                break\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "    \n",
        "    plt.savefig(os.path.join('/content/drive/MyDrive/output_images/', 'image_at_epoch_{:04d}.png'.format(epoch)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9QcD7Kp_yvl"
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return real_loss, fake_loss, total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RULfdy8a_1LM"
      },
      "source": [
        "def generator_d_loss(fake_output):\n",
        "    return tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7_zHZ_U_4gC"
      },
      "source": [
        "def generator_abs_loss(labels, generated_images):\n",
        "    return config.L1_lambda * tf.compat.v1.losses.absolute_difference(labels, generated_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL8PUNGu_7Qe"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVsir1uK_-kV"
      },
      "source": [
        "# create a line plot of loss for the gan and save to file\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(d1_hist, d2_hist, g_hist):\n",
        "\t# plot loss\n",
        " plt.subplot(2, 1, 1)\n",
        " plt.plot(d1_hist, label='d-real')\n",
        " plt.plot(d2_hist, label='d-fake')\n",
        " plt.plot(g_hist, label='gen')\n",
        " plt.legend()\n",
        " plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtcQ0seBABj4"
      },
      "source": [
        "d1_hist, d2_hist, g_hist, a1_hist, a2_hist = list(), list(), list(), list(), list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPK4hdq1AFpm"
      },
      "source": [
        "# This annotation causes the function to be \"compiled\".\n",
        "# @tf.function\n",
        "def train_step(inputs, labels, epoch):\n",
        "\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator_large(inputs, training=True)\n",
        "\n",
        "      real_output = discriminator(labels, training=True)\n",
        "      generated_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_d_loss = generator_d_loss(generated_output)\n",
        "      gen_abs_loss = generator_abs_loss(labels, generated_images)\n",
        "      gen_loss = gen_d_loss + gen_abs_loss\n",
        "\n",
        "\n",
        "      gen_rmse = rmse(labels, generated_images)\n",
        "      gen_psnr = psnr(labels, generated_images)\n",
        "      real_loss, fake_loss, disc_loss = discriminator_loss(real_output, generated_output)\n",
        "      \n",
        "      d1_hist.append(real_loss)\n",
        "      d2_hist.append(fake_loss)\n",
        "      g_hist.append(gen_loss)\n",
        "\n",
        "      print(\"Generator loss: \",gen_loss)\n",
        "      print(\"Discriminator loss: \", disc_loss)\n",
        "\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator_large.trainable_weights)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_weights)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator_large.trainable_weights))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_weights))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTGj9x51AIrc"
      },
      "source": [
        "import time\n",
        "def train(train_dataset, epochs):\n",
        "\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    num_loop = 0\n",
        "    for x_train_input, x_train_label in train_dataset:\n",
        "      if ( num_loop != 5 ):\n",
        "        train_step(x_train_input, x_train_label, epoch)\n",
        "      else:\n",
        "        break\n",
        "      num_loop += 1\n",
        "\n",
        "    \n",
        "    num_examples_to_generate = 16\n",
        "    random_indices = np.random.choice(np.arange(test_X.shape[0]), num_examples_to_generate, replace=False)\n",
        "\n",
        "    selected_inputs = test_X[random_indices]\n",
        "    selected_labels = test_y[random_indices]\n",
        "\n",
        "    generate_and_save_images(generator_large, epoch + 1,selected_inputs,selected_labels)\n",
        "\n",
        "    print ('Time taken for epoch =========================================== {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  generate_and_save_images(generator_large,epochs,selected_inputs,selected_labels)\n",
        "  plot_history(d1_hist, d2_hist, g_hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQPTn-3UANew"
      },
      "source": [
        "# train(train_inputs, train_labels, 20) \n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_X, train_y)).shuffle(config.buffer_size).batch(config.batch_size)\n",
        "train(train_dataset, config.max_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPpxODVWASnQ"
      },
      "source": [
        "plot_history(d1_hist, d2_hist, g_hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdYfIfTqAS3l"
      },
      "source": [
        "rmse_list = list()\n",
        "psnr_list = list()\n",
        "\n",
        "rmse_array = np.array(rmse_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQRYJ8MEAU6p"
      },
      "source": [
        "psnr_array = np.array(psnr_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1IMWFVIAbqU"
      },
      "source": [
        "print(\"mean of rmse: \", np.mean(psnr_array))\n",
        "print(\"mean of psnr: \", np.mean(psnr_array))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfrKmLP4AeGg"
      },
      "source": [
        "print(\"mean of rmse: \", np.std(psnr_array))\n",
        "print(\"mean of psnr: \", np.std(psnr_array))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}